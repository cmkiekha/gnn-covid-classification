{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/04_validation.ipynb\n",
    "\"\"\"\n",
    "Validation Analysis Notebook\n",
    "Purpose: Validate generated samples and assess model performance\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from scipy import stats\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.evaluation.metrics import EvaluationMetrics\n",
    "from src.data.preprocessing import DataProcessor\n",
    "\n",
    "# Load results\n",
    "results_path = sorted(Path('results/models').glob('wgan_model_*.pt'))[-1]\n",
    "results = torch.load(results_path)\n",
    "\n",
    "# Load original data\n",
    "processor = DataProcessor()\n",
    "original_data = pd.read_csv('data/data_combined_controls.csv')\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = EvaluationMetrics()\n",
    "\n",
    "# 1. Statistical Validation\n",
    "print(\"=== Statistical Validation ===\")\n",
    "stats_comparison = evaluator.compare_statistics(\n",
    "    original_data,\n",
    "    results['generated_samples']\n",
    ")\n",
    "print(\"\\nStatistical Comparisons:\")\n",
    "print(stats_comparison)\n",
    "\n",
    "# 2. Distribution Analysis\n",
    "evaluator.visualize_distributions(\n",
    "    original_data,\n",
    "    results['generated_samples']\n",
    ")\n",
    "\n",
    "# 3. Feature-wise Analysis\n",
    "print(\"\\n=== Feature-wise Analysis ===\")\n",
    "for feature in original_data.columns[:5]:  # First 5 features\n",
    "    print(f\"\\nFeature: {feature}\")\n",
    "    print(\"Original mean:\", original_data[feature].mean())\n",
    "    print(\"Generated mean:\", results['generated_samples'][feature].mean())\n",
    "    print(\"Original std:\", original_data[feature].std())\n",
    "    print(\"Generated std:\", results['generated_samples'][feature].std())\n",
    "\n",
    "# 4. Cross-validation Results\n",
    "print(\"\\n=== Cross-validation Results ===\")\n",
    "cv_results = results['cv_results']\n",
    "for fold, metrics in cv_results.items():\n",
    "    print(f\"\\nFold {fold}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# 5. Quality Metrics\n",
    "print(\"\\n=== Quality Metrics ===\")\n",
    "mmd_score = evaluator.compute_mmd(\n",
    "    original_data.values,\n",
    "    results['generated_samples'].values\n",
    ")\n",
    "print(f\"MMD Score: {mmd_score:.4f}\")\n",
    "\n",
    "# 6. Visualization\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "evaluator.generate_tsne(original_data, results['generated_samples'])\n",
    "plt.title('t-SNE Visualization')\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.boxplot(data=[\n",
    "    original_data.mean(),\n",
    "    results['generated_samples'].mean()\n",
    "])\n",
    "plt.title('Feature Means Comparison')\n",
    "plt.xticks([0, 1], ['Original', 'Generated'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save validation results\n",
    "validation_results = {\n",
    "    'statistical_comparison': stats_comparison,\n",
    "    'mmd_score': mmd_score,\n",
    "    'cv_results': cv_results\n",
    "}\n",
    "\n",
    "with open('results/validation_results.pkl', 'wb') as f:\n",
    "    pickle.dump(validation_results, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
