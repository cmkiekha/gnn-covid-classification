{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/enhanced/Validation_Results.ipynb\n",
    "\n",
    "\"\"\"\n",
    "# WGAN-GP Validation Results Analysis\n",
    "Comprehensive validation analysis of the WGAN-GP model with k-fold cross-validation.\n",
    "\n",
    "Contents:\n",
    "1. Cross-Validation Results Analysis\n",
    "2. Generation Quality Assessment\n",
    "3. Feature-wise Analysis\n",
    "4. Statistical Tests\n",
    "5. Visualization of Results\n",
    "\"\"\"\n",
    "\n",
    "# %% Setup and Imports\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import torch\n",
    "\n",
    "from src.utils.enhanced.evaluation import (\n",
    "    compute_statistics,\n",
    "    evaluate_generation,\n",
    "    compute_mmd\n",
    ")\n",
    "\n",
    "# %% Data Loading Functions\n",
    "def load_validation_data():\n",
    "    \"\"\"\n",
    "    Load validation results and generated samples.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing original data, generated samples, and validation metrics\n",
    "    \"\"\"\n",
    "    original_data = pd.read_csv('../../data/original_controls.csv')\n",
    "    generated_data = pd.read_csv('../../results/enhanced/generated_samples.csv')\n",
    "    validation_metrics = pd.read_csv('../../results/enhanced/metrics/validation_metrics.csv')\n",
    "    \n",
    "    return original_data, generated_data, validation_metrics\n",
    "\n",
    "# %% Validation Analysis Functions\n",
    "def analyze_cross_validation_performance(validation_metrics):\n",
    "    \"\"\"\n",
    "    Analyze cross-validation performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        validation_metrics: DataFrame containing validation metrics for each fold\n",
    "    \"\"\"\n",
    "    # Create subplot grid\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    gs = fig.add_gridspec(2, 3)\n",
    "    \n",
    "    # Plot KS statistics distribution\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    sns.boxplot(data=validation_metrics, y='ks_statistic')\n",
    "    ax1.set_title('KS Statistics Across Folds')\n",
    "    \n",
    "    # Plot Wasserstein distances\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    sns.boxplot(data=validation_metrics, y='wasserstein_distance')\n",
    "    ax2.set_title('Wasserstein Distances')\n",
    "    \n",
    "    # Plot MMD scores\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    sns.boxplot(data=validation_metrics, y='mmd_score')\n",
    "    ax3.set_title('MMD Scores')\n",
    "    \n",
    "    # Plot correlation preservation\n",
    "    ax4 = fig.add_subplot(gs[1, :])\n",
    "    sns.lineplot(data=validation_metrics, x='fold', y='correlation_preservation')\n",
    "    ax4.set_title('Feature Correlation Preservation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_feature_stability(original_data, generated_samples_list):\n",
    "    \"\"\"\n",
    "    Analyze feature stability across different folds.\n",
    "    \n",
    "    Args:\n",
    "        original_data: Original control samples\n",
    "        generated_samples_list: List of generated samples from each fold\n",
    "    \"\"\"\n",
    "    feature_stats = []\n",
    "    \n",
    "    for i, samples in enumerate(generated_samples_list):\n",
    "        stats_dict = {\n",
    "            'fold': i,\n",
    "            'mean_diff': np.mean(np.abs(samples.mean() - original_data.mean())),\n",
    "            'std_diff': np.mean(np.abs(samples.std() - original_data.std())),\n",
    "            'correlation': np.corrcoef(samples.mean(), original_data.mean())[0,1]\n",
    "        }\n",
    "        feature_stats.append(stats_dict)\n",
    "    \n",
    "    stats_df = pd.DataFrame(feature_stats)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    sns.barplot(data=stats_df, x='fold', y='mean_diff', ax=axes[0])\n",
    "    axes[0].set_title('Mean Difference by Fold')\n",
    "    \n",
    "    sns.barplot(data=stats_df, x='fold', y='std_diff', ax=axes[1])\n",
    "    axes[1].set_title('STD Difference by Fold')\n",
    "    \n",
    "    sns.barplot(data=stats_df, x='fold', y='correlation', ax=axes[2])\n",
    "    axes[2].set_title('Feature Correlation by Fold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_sample_quality(original_data, generated_data):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of sample quality.\n",
    "    \n",
    "    Args:\n",
    "        original_data: Original control samples\n",
    "        generated_data: Generated synthetic samples\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    gs = fig.add_gridspec(2, 4)\n",
    "    \n",
    "    # t-SNE visualization\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    combined_data = pd.concat([original_data, generated_data])\n",
    "    tsne_results = tsne.fit_transform(combined_data)\n",
    "    \n",
    "    ax1.scatter(tsne_results[:len(original_data), 0],\n",
    "                tsne_results[:len(original_data), 1],\n",
    "                label='Original', alpha=0.7)\n",
    "    ax1.scatter(tsne_results[len(original_data):, 0],\n",
    "                tsne_results[len(original_data):, 1],\n",
    "                label='Generated', alpha=0.7)\n",
    "    ax1.set_title('t-SNE Visualization')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # PCA visualization\n",
    "    ax2 = fig.add_subplot(gs[0, 2:])\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_results = pca.fit_transform(combined_data)\n",
    "    \n",
    "    ax2.scatter(pca_results[:len(original_data), 0],\n",
    "                pca_results[:len(original_data), 1],\n",
    "                label='Original', alpha=0.7)\n",
    "    ax2.scatter(pca_results[len(original_data):, 0],\n",
    "                pca_results[len(original_data):, 1],\n",
    "                label='Generated', alpha=0.7)\n",
    "    ax2.set_title('PCA Visualization')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Feature distributions\n",
    "    for i, feature in enumerate(np.random.choice(original_data.columns, 4)):\n",
    "        ax = fig.add_subplot(gs[1, i])\n",
    "        sns.kdeplot(data=original_data[feature], ax=ax, label='Original')\n",
    "        sns.kdeplot(data=generated_data[feature], ax=ax, label='Generated')\n",
    "        ax.set_title(f'Feature: {feature}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %% Statistical Analysis Functions\n",
    "def perform_statistical_tests(original_data, generated_data):\n",
    "    \"\"\"\n",
    "    Perform comprehensive statistical tests on the data.\n",
    "    \n",
    "    Args:\n",
    "        original_data: Original control samples\n",
    "        generated_data: Generated synthetic samples\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'feature': [],\n",
    "        'ks_statistic': [],\n",
    "        'ks_pvalue': [],\n",
    "        'mean_diff': [],\n",
    "        'std_diff': [],\n",
    "        'correlation': []\n",
    "    }\n",
    "    \n",
    "    for col in original_data.columns:\n",
    "        # KS test\n",
    "        ks_stat, p_val = stats.ks_2samp(original_data[col], generated_data[col])\n",
    "        \n",
    "        # Basic statistics\n",
    "        mean_diff = abs(original_data[col].mean() - generated_data[col].mean())\n",
    "        std_diff = abs(original_data[col].std() - generated_data[col].std())\n",
    "        correlation = np.corrcoef(original_data[col], generated_data[col])[0,1]\n",
    "        \n",
    "        results['feature'].append(col)\n",
    "        results['ks_statistic'].append(ks_stat)\n",
    "        results['ks_pvalue'].append(p_val)\n",
    "        results['mean_diff'].append(mean_diff)\n",
    "        results['std_diff'].append(std_diff)\n",
    "        results['correlation'].append(correlation)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# %% Run Analysis\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    original_data, generated_data, validation_metrics = load_validation_data()\n",
    "    \n",
    "    # Analyze cross-validation performance\n",
    "    print(\"Analyzing cross-validation performance...\")\n",
    "    analyze_cross_validation_performance(validation_metrics)\n",
    "    \n",
    "    # Analyze feature stability\n",
    "    print(\"\\nAnalyzing feature stability...\")\n",
    "    analyze_feature_stability(original_data, generated_data)\n",
    "    \n",
    "    # Visualize sample quality\n",
    "    print(\"\\nVisualizing sample quality...\")\n",
    "    visualize_sample_quality(original_data, generated_data)\n",
    "    \n",
    "    # Perform statistical tests\n",
    "    print(\"\\nPerforming statistical tests...\")\n",
    "    statistical_results = perform_statistical_tests(original_data, generated_data)\n",
    "    print(\"\\nStatistical Test Summary:\")\n",
    "    print(statistical_results.describe())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
