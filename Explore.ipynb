{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# from src.models.data_augmentation.VAE import *\n",
    "# from src.models.data_augmentation.WAE import *\n",
    "from src.models.data_augmentation.GAN import *\n",
    "from src.utils.evaluation import *\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"data/data_combined_controls.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying Explore.ipynb to Bypass AE and WAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the start of the notebook\n",
    "run_models = {'VAE': False, 'WAE': False, 'GAN': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "dataset, tensor_data, scaled_data, scaler, original_dim = process(dataset_path)\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, tensor_data, scaled_data, scaler, original_dim = process(dataset_path)\n",
    "\n",
    "# vae = train_vae(dataset, original_dim)\n",
    "# augmented_df = generate_vae(vae, scaled_data.columns, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_stats_df = compare_statistics(scaled_data, augmented_df)\n",
    "# compare_distributions_df = compare_distributions(scaled_data, augmented_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 6))\n",
    "# sns.histplot(compare_distributions_df['KS Statistic'], kde=True)\n",
    "# plt.title('VAE - Distribution of KS Statistics for Original vs. Synthetic Data')\n",
    "# plt.xlabel('KS Statistic')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_tsne(scaled_data, augmented_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, tensor_data, scaled_data, scaler, original_dim = process(dataset_path)\n",
    "\n",
    "# wae = train_wae(dataset, original_dim)\n",
    "# augmented_df = generate_wae(wae, scaled_data.columns, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_stats_df = compare_statistics(scaled_data, augmented_df)\n",
    "# compare_distributions_df = compare_distributions(scaled_data, augmented_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 6))\n",
    "# sns.histplot(compare_distributions_df['KS Statistic'], kde=True)\n",
    "# plt.title('Distribution of KS Statistics for Original vs. Synthetic Data')\n",
    "# plt.xlabel('KS Statistic')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_tsne(scaled_data, augmented_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting WGAN-GP training with K-fold validation...\n",
      "\n",
      "Dataset information:\n",
      "Original data shape: (23, 8064)\n",
      "Number of features: 8063\n",
      "\n",
      "Column types:\n",
      "IGKV2.28     float64\n",
      "IGKV3D.20    float64\n",
      "IGKV1.12     float64\n",
      "IGLC7        float64\n",
      "IGKV2.29     float64\n",
      "              ...   \n",
      "ZSWIM8       float64\n",
      "ZW10         float64\n",
      "ZWILCH       float64\n",
      "ZWINT        float64\n",
      "Label         object\n",
      "Length: 8064, dtype: object\n",
      "\n",
      "Training parameters:\n",
      "Epochs: 20\n",
      "Batch size: 32\n",
      "Learning rate: 0.001\n",
      "Device: cpu\n",
      "Number of folds: 5\n",
      "\n",
      "Training WGAN-GP and generating synthetic samples...\n",
      "\n",
      "Error occurred during execution:\n",
      "Error type: TypeError\n",
      "Error message: train_and_generate() got an unexpected keyword argument 'learning_rate'\n",
      "\n",
      "Full traceback:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/xr/bn1vhjh5559gb9ws4vntkqj80000gn/T/ipykernel_62759/578749626.py\", line 40, in <module>\n",
      "    generated_samples = train_and_generate(\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: train_and_generate() got an unexpected keyword argument 'learning_rate'\n"
     ]
    }
   ],
   "source": [
    "## WGAN-GP Model\n",
    "\n",
    "print(\"Starting WGAN-GP training with K-fold validation...\\n\")\n",
    "\n",
    "# Load and process data\n",
    "dataset_path = \"data/data_combined_controls.csv\"\n",
    "dataset, tensor_data, scaled_data, scaler, original_dim = process(dataset_path)\n",
    "\n",
    "# Print initial data information\n",
    "print(\"Dataset information:\")\n",
    "print(f\"Original data shape: {scaled_data.shape}\")\n",
    "print(f\"Number of features: {original_dim}\")\n",
    "print(\"\\nColumn types:\")\n",
    "print(scaled_data.dtypes)\n",
    "\n",
    "# Set training parameters\n",
    "epochs = 20  # Reduced for debugging\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_splits = 5\n",
    "\n",
    "print(f\"\\nTraining parameters:\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Number of folds: {n_splits}\")\n",
    "\n",
    "try:\n",
    "    # Generate synthetic samples using WGAN-GP with k-fold validation\n",
    "    print(\"\\nTraining WGAN-GP and generating synthetic samples...\")\n",
    "    generated_samples = train_and_generate(\n",
    "        filepath=dataset_path,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        device=device,\n",
    "        n_splits=n_splits,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame with generated samples\n",
    "    print(\"\\nProcessing generated samples...\")\n",
    "    generated_df = pd.DataFrame(generated_samples, columns=scaled_data.columns)\n",
    "    \n",
    "    # Inverse transform the generated samples to original scale\n",
    "    df_unscaled = pd.DataFrame(\n",
    "        scaler.inverse_transform(generated_df.drop(['fold', 'type'], axis=1)),\n",
    "        columns=[col for col in scaled_data.columns if col not in ['fold', 'type']]\n",
    "    )\n",
    "    \n",
    "    # Calculate KS statistics\n",
    "    print(\"\\nCalculating KS statistics...\")\n",
    "    ks_stats = compare_distributions(scaled_data, df_unscaled)\n",
    "    \n",
    "    # Plot KS statistics distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(ks_stats['KS Statistic'], kde=True)\n",
    "    plt.title('Distribution of KS Statistics: Original vs. Synthetic Data')\n",
    "    plt.xlabel('KS Statistic')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and plot augmented data with variance\n",
    "    print(\"\\nApplying recentering...\")\n",
    "    augmented_data_with_variance = recenter_data(df_unscaled, scaled_data)\n",
    "    \n",
    "    # Calculate KS statistics after recentering\n",
    "    ks_stats_with_added_variance = compare_distributions(scaled_data, augmented_data_with_variance)\n",
    "    \n",
    "    # Plot KS statistics after recentering\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(ks_stats_with_added_variance['KS Statistic'], kde=True)\n",
    "    plt.title('Distribution of KS Statistics After Recentering')\n",
    "    plt.xlabel('KS Statistic')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate t-SNE visualization\n",
    "    print(\"\\nGenerating t-SNE visualization...\")\n",
    "    generate_tsne(scaled_data, augmented_data_with_variance)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(\"\\nBefore recentering:\")\n",
    "    print(ks_stats['KS Statistic'].describe())\n",
    "    print(\"\\nAfter recentering:\")\n",
    "    print(ks_stats_with_added_variance['KS Statistic'].describe())\n",
    "    \n",
    "    # Print generation statistics\n",
    "    print(f\"\\nTotal synthetic samples generated: {len(generated_df)}\")\n",
    "    print(f\"Samples per fold: {len(generated_df) // n_splits}\")\n",
    "    \n",
    "    # Compare mean and std between original and synthetic data\n",
    "    print(\"\\nFeature Statistics Comparison:\")\n",
    "    for column in scaled_data.columns:\n",
    "        print(f\"\\n{column}:\")\n",
    "        print(f\"Original - Mean: {scaled_data[column].mean():.4f}, Std: {scaled_data[column].std():.4f}\")\n",
    "        print(f\"Synthetic - Mean: {augmented_data_with_variance[column].mean():.4f}, Std: {augmented_data_with_variance[column].std():.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError occurred during execution:\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    print(\"\\nFull traceback:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE BELOW GENERATING ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, tensor_data, scaled_data, scaler, original_dim = process(dataset_path)\n",
    "\n",
    "# # Parameter designation for Debugging\n",
    "# epochs = 20  # Small number for initial debugging\n",
    "# batch_size = 32  # A reasonable starting point\n",
    "# learning_rate = 0.001  # Typical for many applications\n",
    "\n",
    "# generated_samples = train_and_generate(dataset_path, batch_size=batch_size, epochs=epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_df = pd.DataFrame(generated_samples, columns=scaled_data.columns)\n",
    "# df_unscaled = pd.DataFrame(scaler.inverse_transform(generated_df), columns=generated_df.columns)\n",
    "# ks_stats = compare_distributions(scaled_data, df_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 6))\n",
    "# sns.histplot(ks_stats['KS Statistic'], kde=True)\n",
    "# plt.title('Distribution of KS Statistics for Original vs. Synthetic Data')\n",
    "# plt.xlabel('KS Statistic')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_data_with_variance = recenter_data(df_unscaled, scaled_data)\n",
    "# ks_stats_with_added_variance = compare_distributions(scaled_data, augmented_data_with_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 6))\n",
    "# sns.histplot(ks_stats_with_added_variance['KS Statistic'], kde=True)\n",
    "# plt.title('Distribution of KS Statistics for Original vs. Synthetic Data')\n",
    "# plt.xlabel('KS Statistic')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_tsne(scaled_data, augmented_data_with_variance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
