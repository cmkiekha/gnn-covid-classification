{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# from src.models.data_augmentation.VAE import *\n",
    "# from src.models.data_augmentation.WAE import *\n",
    "from src.models.data_augmentation.GAN import *\n",
    "from src.utils.evaluation import *\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"data/data_combined_controls.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying Explore.ipynb to Bypass AE and WAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the start of the notebook\n",
    "run_models = {'VAE': False, 'WAE': False, 'GAN': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "dataset, tensor_data, scaled_data, scaler, original_dim = process(dataset_path)\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, tensor_data, scaled_data, scaler, original_dim = process(dataset_path)\n",
    "\n",
    "# vae = train_vae(dataset, original_dim)\n",
    "# augmented_df = generate_vae(vae, scaled_data.columns, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_stats_df = compare_statistics(scaled_data, augmented_df)\n",
    "# compare_distributions_df = compare_distributions(scaled_data, augmented_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 6))\n",
    "# sns.histplot(compare_distributions_df['KS Statistic'], kde=True)\n",
    "# plt.title('VAE - Distribution of KS Statistics for Original vs. Synthetic Data')\n",
    "# plt.xlabel('KS Statistic')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_tsne(scaled_data, augmented_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, tensor_data, scaled_data, scaler, original_dim = process(dataset_path)\n",
    "\n",
    "# wae = train_wae(dataset, original_dim)\n",
    "# augmented_df = generate_wae(wae, scaled_data.columns, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_stats_df = compare_statistics(scaled_data, augmented_df)\n",
    "# compare_distributions_df = compare_distributions(scaled_data, augmented_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 6))\n",
    "# sns.histplot(compare_distributions_df['KS Statistic'], kde=True)\n",
    "# plt.title('Distribution of KS Statistics for Original vs. Synthetic Data')\n",
    "# plt.xlabel('KS Statistic')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_tsne(scaled_data, augmented_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting WGAN-GP training with K-fold validation...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset information:\n",
      "Original data shape: (23, 8063)\n",
      "Number of features: 8063\n",
      "\n",
      "Column types:\n",
      "IGKV2.28     float64\n",
      "IGKV3D.20    float64\n",
      "IGKV1.12     float64\n",
      "IGLC7        float64\n",
      "IGKV2.29     float64\n",
      "              ...   \n",
      "ZSCAN32      float64\n",
      "ZSWIM8       float64\n",
      "ZW10         float64\n",
      "ZWILCH       float64\n",
      "ZWINT        float64\n",
      "Length: 8063, dtype: object\n",
      "\n",
      "Training parameters:\n",
      "Epochs: 20\n",
      "Batch size: 32\n",
      "Learning rate: 0.001\n",
      "Device: cpu\n",
      "Number of folds: 5\n",
      "\n",
      "Training WGAN-GP and generating synthetic samples...\n",
      "\n",
      "Data Overview:\n",
      "Original samples: 23\n",
      "Features: 8063\n",
      "\n",
      "Feature types:\n",
      "IGKV2.28     float64\n",
      "IGKV3D.20    float64\n",
      "IGKV1.12     float64\n",
      "IGLC7        float64\n",
      "IGKV2.29     float64\n",
      "              ...   \n",
      "ZSCAN32      float64\n",
      "ZSWIM8       float64\n",
      "ZW10         float64\n",
      "ZWILCH       float64\n",
      "ZWINT        float64\n",
      "Length: 8063, dtype: object\n",
      "\n",
      "Using device: cpu\n",
      "\n",
      "Processing fold 1/5\n",
      "Training samples: 18\n",
      "Validation samples: 5\n",
      "Train Loader Len: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training WGAN-GP:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20] Batch [0] G_loss: 27.6189 C_loss: -118.3356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training WGAN-GP:   5%|▌         | 1/20 [02:14<42:29, 134.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Summary:\n",
      "Average G_loss: 16.9546\n",
      "Average C_loss: -312.6441\n",
      "Epoch [1/20] Batch [0] G_loss: 5.0891 C_loss: -51.8142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training WGAN-GP:  10%|█         | 2/20 [04:29<40:27, 134.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Average G_loss: 34.3543\n",
      "Average C_loss: -390.6550\n",
      "Epoch [2/20] Batch [0] G_loss: 8.1996 C_loss: -87.8171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training WGAN-GP:  15%|█▌        | 3/20 [06:46<38:27, 135.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Average G_loss: 20.3857\n",
      "Average C_loss: -444.1220\n",
      "Epoch [3/20] Batch [0] G_loss: 1.5772 C_loss: 0.9374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training WGAN-GP:  20%|██        | 4/20 [09:01<36:11, 135.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Average G_loss: 27.6542\n",
      "Average C_loss: -453.4451\n",
      "Epoch [4/20] Batch [0] G_loss: 37.6819 C_loss: -26.7995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training WGAN-GP:  25%|██▌       | 5/20 [11:18<34:00, 136.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Average G_loss: 18.5887\n",
      "Average C_loss: -395.6305\n",
      "Epoch [5/20] Batch [0] G_loss: -15.0549 C_loss: -42.2535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training WGAN-GP:  30%|███       | 6/20 [13:34<31:45, 136.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Average G_loss: 11.2747\n",
      "Average C_loss: -379.8270\n",
      "Epoch [6/20] Batch [0] G_loss: -16.4635 C_loss: 43.2421\n"
     ]
    }
   ],
   "source": [
    "## WGAN-GP Model\n",
    "\n",
    "print(\"Starting WGAN-GP training with K-fold validation...\\n\")\n",
    "\n",
    "# Load and process data\n",
    "dataset_path = \"data/data_combined_controls.csv\"\n",
    "dataset, tensor_data, scaled_data, scaler, original_dim = process(dataset_path)\n",
    "\n",
    "# Print initial data information\n",
    "print(\"Dataset information:\")\n",
    "print(f\"Original data shape: {scaled_data.shape}\")\n",
    "print(f\"Number of features: {original_dim}\")\n",
    "print(\"\\nColumn types:\")\n",
    "print(scaled_data.dtypes)\n",
    "\n",
    "# Set training parameters\n",
    "epochs = 20  # Reduced for debugging\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_splits = 5\n",
    "\n",
    "print(f\"\\nTraining parameters:\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Number of folds: {n_splits}\")\n",
    "\n",
    "try:\n",
    "    # Generate synthetic samples using WGAN-GP with k-fold validation\n",
    "    print(\"\\nTraining WGAN-GP and generating synthetic samples...\")\n",
    "    generated_samples = train_and_generate(\n",
    "        filepath=dataset_path,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        device=device,\n",
    "        n_splits=n_splits,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame with generated samples\n",
    "    print(\"\\nProcessing generated samples...\")\n",
    "    generated_df = pd.DataFrame(generated_samples, columns=scaled_data.columns)\n",
    "    \n",
    "    # Inverse transform the generated samples to original scale\n",
    "    df_unscaled = pd.DataFrame(\n",
    "        scaler.inverse_transform(generated_df.drop(['fold', 'type'], axis=1)),\n",
    "        columns=[col for col in scaled_data.columns if col not in ['fold', 'type']]\n",
    "    )\n",
    "    \n",
    "    # Calculate KS statistics\n",
    "    print(\"\\nCalculating KS statistics...\")\n",
    "    ks_stats = compare_distributions(scaled_data, df_unscaled)\n",
    "    \n",
    "    # Plot KS statistics distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(ks_stats['KS Statistic'], kde=True)\n",
    "    plt.title('Distribution of KS Statistics: Original vs. Synthetic Data')\n",
    "    plt.xlabel('KS Statistic')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and plot augmented data with variance\n",
    "    print(\"\\nApplying recentering...\")\n",
    "    augmented_data_with_variance = recenter_data(df_unscaled, scaled_data)\n",
    "    \n",
    "    # Calculate KS statistics after recentering\n",
    "    ks_stats_with_added_variance = compare_distributions(scaled_data, augmented_data_with_variance)\n",
    "    \n",
    "    # Plot KS statistics after recentering\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(ks_stats_with_added_variance['KS Statistic'], kde=True)\n",
    "    plt.title('Distribution of KS Statistics After Recentering')\n",
    "    plt.xlabel('KS Statistic')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate t-SNE visualization\n",
    "    print(\"\\nGenerating t-SNE visualization...\")\n",
    "    generate_tsne(scaled_data, augmented_data_with_variance)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(\"\\nBefore recentering:\")\n",
    "    print(ks_stats['KS Statistic'].describe())\n",
    "    print(\"\\nAfter recentering:\")\n",
    "    print(ks_stats_with_added_variance['KS Statistic'].describe())\n",
    "    \n",
    "    # Print generation statistics\n",
    "    print(f\"\\nTotal synthetic samples generated: {len(generated_df)}\")\n",
    "    print(f\"Samples per fold: {len(generated_df) // n_splits}\")\n",
    "    \n",
    "    # Compare mean and std between original and synthetic data\n",
    "    print(\"\\nFeature Statistics Comparison:\")\n",
    "    for column in scaled_data.columns:\n",
    "        print(f\"\\n{column}:\")\n",
    "        print(f\"Original - Mean: {scaled_data[column].mean():.4f}, Std: {scaled_data[column].std():.4f}\")\n",
    "        print(f\"Synthetic - Mean: {augmented_data_with_variance[column].mean():.4f}, Std: {augmented_data_with_variance[column].std():.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError occurred during execution:\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    print(\"\\nFull traceback:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE BELOW GENERATING ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, tensor_data, scaled_data, scaler, original_dim = process(dataset_path)\n",
    "\n",
    "# # Parameter designation for Debugging\n",
    "# epochs = 20  # Small number for initial debugging\n",
    "# batch_size = 32  # A reasonable starting point\n",
    "# learning_rate = 0.001  # Typical for many applications\n",
    "\n",
    "# generated_samples = train_and_generate(dataset_path, batch_size=batch_size, epochs=epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_df = pd.DataFrame(generated_samples, columns=scaled_data.columns)\n",
    "# df_unscaled = pd.DataFrame(scaler.inverse_transform(generated_df), columns=generated_df.columns)\n",
    "# ks_stats = compare_distributions(scaled_data, df_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 6))\n",
    "# sns.histplot(ks_stats['KS Statistic'], kde=True)\n",
    "# plt.title('Distribution of KS Statistics for Original vs. Synthetic Data')\n",
    "# plt.xlabel('KS Statistic')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_data_with_variance = recenter_data(df_unscaled, scaled_data)\n",
    "# ks_stats_with_added_variance = compare_distributions(scaled_data, augmented_data_with_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 6))\n",
    "# sns.histplot(ks_stats_with_added_variance['KS Statistic'], kde=True)\n",
    "# plt.title('Distribution of KS Statistics for Original vs. Synthetic Data')\n",
    "# plt.xlabel('KS Statistic')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_tsne(scaled_data, augmented_data_with_variance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
