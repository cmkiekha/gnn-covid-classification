{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Verification\n",
      "--------------------------------------------------\n",
      "✓ NumPy                     (numpy        version: 1.26.4)\n",
      "✓ Pandas                    (pandas       version: 2.2.3)\n",
      "✓ Plotting                  (matplotlib   version: 3.9.2)\n",
      "✓ Statistical Visualization (seaborn      version: 0.13.2)\n",
      "✓ Scientific Computing      (scipy        version: 1.14.1)\n",
      "✓ Machine Learning          (sklearn      version: 1.5.1)\n",
      "✓ PyTorch                   (torch        version: 2.5.1)\n",
      "✓ Progress Bars             (tqdm         version: 4.66.6)\n",
      "✓ Jupyter Support           (ipykernel    version: 6.29.5)\n",
      "\n",
      "Python version: 3.11.8\n",
      "✓ Plotting configuration successful\n",
      "\n",
      "Environment setup complete. Ready to proceed with analysis.\n",
      "\n",
      "Configuration Settings:\n",
      "--------------------------------------------------\n",
      "Data path: data/data_combined_controls.csv\n",
      "Results directory: results/20241102_194444\n",
      "\n",
      "Model Parameters:\n",
      "  batch_size: 32\n",
      "  epochs: 5\n",
      "  random_state: 42\n",
      "  n_splits: 3\n",
      "  device: cpu\n",
      "\n",
      "Visualization Parameters:\n",
      "  figsize: (12, 8)\n",
      "  dpi: 300\n",
      "  style: whitegrid\n",
      "\n",
      "Step 1: Data Loading and Preprocessing\n",
      "--------------------------------------------------\n",
      "\n",
      "Loading and processing data...\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Documentation and Imports\n",
    "\"\"\"\n",
    "WGAN-GP Training and Evaluation Notebook for COVID-19 Control Data Augmentation\n",
    "\n",
    "This notebook implements a Wasserstein GAN with Gradient Penalty (WGAN-GP) for generating\n",
    "synthetic COVID-19 control samples. The implementation focuses on data quality and validation.\n",
    "\n",
    "Key Components:\n",
    "1. Data Loading and Preprocessing\n",
    "   - Target column identification ('group' or 'target')\n",
    "   - Data validation and cleaning\n",
    "   - Feature scaling and normalization\n",
    "\n",
    "2. WGAN-GP Training with k-fold Cross Validation\n",
    "   - K-fold validation (k=3) for robust evaluation\n",
    "   - Gradient penalty for Wasserstein distance\n",
    "   - Dynamic batch processing\n",
    "\n",
    "3. Synthetic Data Evaluation\n",
    "   - Statistical distribution matching\n",
    "   - Feature-wise comparisons\n",
    "   - Quality metrics and visualization\n",
    "\n",
    "4. Analysis and Reporting\n",
    "   - Comprehensive quality metrics\n",
    "   - Visual comparisons\n",
    "   - Detailed reporting\n",
    "\n",
    "Author: CM Kiekhaefer\n",
    "Date: 11-02-2024\n",
    "Version: 1.0\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "# Define core data structure\n",
    "DataInfo = namedtuple('DataInfo', [\n",
    "    'dataset',      # Original dataset\n",
    "    'tensor_data',  # Data in tensor format\n",
    "    'scaled_data',  # Scaled data\n",
    "    'scaler',       # Fitted scaler object\n",
    "    'n_features'    # Number of features\n",
    "])\n",
    "# Cell 2: Environment Setup and Verification\n",
    "def verify_environment():\n",
    "    \"\"\"\n",
    "    Verify all required package installations and versions.\n",
    "    \n",
    "    Checks:\n",
    "    1. Core packages: numpy, pandas, torch\n",
    "    2. Visualization: matplotlib, seaborn\n",
    "    3. Machine learning: scikit-learn\n",
    "    4. Support packages: tqdm, ipykernel\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if all required packages are available\n",
    "    \"\"\"\n",
    "    print(\"Environment Verification\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    packages = {\n",
    "        'numpy': 'NumPy',\n",
    "        'pandas': 'Pandas',\n",
    "        'matplotlib': 'Plotting',\n",
    "        'seaborn': 'Statistical Visualization',\n",
    "        'scipy': 'Scientific Computing',\n",
    "        'sklearn': 'Machine Learning',\n",
    "        'torch': 'PyTorch',\n",
    "        'tqdm': 'Progress Bars',\n",
    "        'ipykernel': 'Jupyter Support'\n",
    "    }\n",
    "    \n",
    "    all_passed = True\n",
    "    for package, description in packages.items():\n",
    "        try:\n",
    "            module = __import__(package)\n",
    "            version = getattr(module, '__version__', 'unknown')\n",
    "            print(f\"✓ {description:<25} ({package:<12} version: {version})\")\n",
    "        except ImportError as e:\n",
    "            all_passed = False\n",
    "            print(f\"✗ {description:<25} ({package:<12} ERROR: {str(e)})\")\n",
    "    \n",
    "    print(f\"\\nPython version: {sys.version.split()[0]}\")\n",
    "    return all_passed\n",
    "\n",
    "# Cell 3: Package Imports and Setup (only if verification passes)\n",
    "if verify_environment():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Import local modules\n",
    "    import config\n",
    "    from src.models.data_augmentation.GAN_v2 import train_and_generate\n",
    "    from src.utils.preprocessing_v2 import process\n",
    "    from src.utils.evaluation_v2 import SyntheticDataEvaluator\n",
    "    \n",
    "    # Set plotting configurations\n",
    "    def setup_plotting():\n",
    "        \"\"\"Configure plotting settings.\"\"\"\n",
    "        sns.set_theme(style='whitegrid')\n",
    "        plt.rcParams.update({\n",
    "            'figure.figsize': (12, 8),\n",
    "            'figure.dpi': 300,\n",
    "            'axes.titlesize': 14,\n",
    "            'axes.labelsize': 12,\n",
    "            'xtick.labelsize': 10,\n",
    "            'ytick.labelsize': 10,\n",
    "            'lines.linewidth': 2,\n",
    "            'grid.alpha': 0.3\n",
    "        })\n",
    "        sns.set_palette(\"husl\")\n",
    "        %matplotlib inline\n",
    "        \n",
    "        # Verify plotting setup\n",
    "        plt.figure()\n",
    "        sns.lineplot(x=[1, 2, 3], y=[1, 2, 3])\n",
    "        plt.title(\"Plotting Test\")\n",
    "        plt.close()\n",
    "        print(\"✓ Plotting configuration successful\")\n",
    "    \n",
    "    # Initialize plotting\n",
    "    setup_plotting()\n",
    "    \n",
    "    # Set random seeds\n",
    "    torch.manual_seed(config.RANDOM_STATE)\n",
    "    np.random.seed(config.RANDOM_STATE)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    print(\"\\nEnvironment setup complete. Ready to proceed with analysis.\")\n",
    "else:\n",
    "    raise EnvironmentError(\"Please fix package installation issues before proceeding.\")\n",
    "\n",
    "# Cell 4: Configuration and Data Processing Functions\n",
    "def print_config(config: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Display configuration settings with clear parameter sources.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary containing all parameters\n",
    "    \"\"\"\n",
    "    print(\"\\nConfiguration Settings:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Data path: {config['data_path']}\")\n",
    "    print(f\"Results directory: {config['results_dir']}\")\n",
    "    \n",
    "    print(\"\\nModel Parameters:\")\n",
    "    for key, value in config['model_params'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nVisualization Parameters:\")\n",
    "    for key, value in config['visualization_params'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Cell 4: Configuration\n",
    "def setup_config(is_debug: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Initialize and validate configuration settings.\n",
    "    \n",
    "    Creates a configuration dictionary with:\n",
    "    1. Data paths and directories\n",
    "    2. Model parameters (batch size, epochs, etc.)\n",
    "    3. Cross-validation settings\n",
    "    4. Visualization parameters\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing validated configuration\n",
    "    \n",
    "    Raises:\n",
    "        AssertionError: If configuration validation fails\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = Path(config.RESULT_DIR) / timestamp\n",
    "    \n",
    "    loaded_config = {\n",
    "        'data_path': config.DATA_PATH,\n",
    "        'results_dir': results_dir,\n",
    "        'model_params': {\n",
    "            'batch_size': config.BATCH_SIZE,\n",
    "            'epochs': config.DEBUG_EPOCHS,\n",
    "            'random_state': config.RANDOM_STATE,\n",
    "            'n_splits': config.CV_N_SPLITS,\n",
    "            'device': config.DEVICE\n",
    "        },\n",
    "        'visualization_params': {\n",
    "            'figsize': (12, 8),\n",
    "            'dpi': 300,\n",
    "            'style': 'whitegrid'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if not is_debug:\n",
    "        print(\"Using DEV configs\")\n",
    "        loaded_config['model_params']['epochs'] = config.DEV_EPOCHS\n",
    "    else:\n",
    "        print(\"Using DEBUG configs\")\n",
    "\n",
    "    # Create results directory\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Validate configuration\n",
    "    assert loaded_config['model_params']['batch_size'] == config.BATCH_SIZE, \"Batch size mismatch\"\n",
    "    assert loaded_config['model_params']['n_splits'] == config.CV_N_SPLITS, \"CV splits mismatch\"\n",
    "    assert loaded_config['model_params']['device'] == config.DEVICE, \"Device mismatch\"\n",
    "   \n",
    "    return loaded_config\n",
    "\n",
    "# Cell 5: Data Processing\n",
    "def inspect_data(filepath: str) -> list:\n",
    "    \"\"\"\n",
    "    Inspect and validate input data file.\n",
    "    \n",
    "    Performs:\n",
    "    1. File existence check\n",
    "    2. Column inspection\n",
    "    3. Target column identification\n",
    "    4. Data value validation\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to data file\n",
    "        \n",
    "    Returns:\n",
    "        list: Available column names\n",
    "        \n",
    "    Notes:\n",
    "        Handles both 'group' and 'target' as valid target columns\n",
    "    \"\"\"\n",
    "    # Verify file exists\n",
    "    if not Path(filepath).exists():\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "        \n",
    "    # Read data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    print(\"\\nData Inspection:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Number of rows: {len(df)}\")\n",
    "    print(f\"Number of columns: {len(df.columns)}\")\n",
    "    print(\"\\nColumns found:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  - {col}\")\n",
    "        \n",
    "    # Check for potential target columns\n",
    "    potential_target_cols = [\n",
    "        col for col in df.columns \n",
    "        if col.lower() in ['target', 'group', 'label', 'class', 'status']\n",
    "    ]\n",
    "    \n",
    "    if potential_target_cols:\n",
    "        print(\"\\nPotential target columns found:\")\n",
    "        for col in potential_target_cols:\n",
    "            print(f\"  - {col}\")\n",
    "            if col in df.columns:\n",
    "                print(f\"    Values: {df[col].unique()}\")\n",
    "    else:\n",
    "        print(\"\\nNo obvious target column found\")\n",
    "        \n",
    "    return df.columns.tolist()\n",
    "\n",
    "def validate_and_process_data(filepath: str) -> Tuple[pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Validate data and identify target column.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the data file\n",
    "    Returns:\n",
    "        Tuple of (DataFrame, target_column_name)\n",
    "    \"\"\"\n",
    "    columns = inspect_data(filepath)\n",
    "    assert columns is not None, \"Failed to read data file\"\n",
    "    \n",
    "    # Read data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # First check for 'group' column\n",
    "    if 'group' in df.columns:\n",
    "        target_col = 'group'\n",
    "        print(\"\\nUsing 'group' as target column\")\n",
    "    # Then check for 'target' column\n",
    "    elif 'target' in df.columns:\n",
    "        target_col = 'target'\n",
    "        print(\"\\nUsing 'target' as target column\")\n",
    "    else:\n",
    "        # List potential columns and ask for confirmation\n",
    "        print(\"\\nNo standard target column found. Available columns:\")\n",
    "        for i, col in enumerate(columns):\n",
    "            print(f\"{i}: {col}\")\n",
    "        col_idx = input(\"\\nEnter column index to use as target: \")\n",
    "        assert col_idx.isdigit() and int(col_idx) < len(columns), \"Invalid column index\"\n",
    "        target_col = columns[int(col_idx)]\n",
    "    \n",
    "    # Verify target column values\n",
    "    unique_values = df[target_col].unique()\n",
    "    assert len(unique_values) == 2, f\"Target column must be binary, found values: {unique_values}\"\n",
    "    \n",
    "    # Rename target column if needed\n",
    "    if target_col != 'target':\n",
    "        df = df.rename(columns={target_col: 'target'})\n",
    "        print(f\"\\nRenamed column '{target_col}' to 'target'\")\n",
    "    \n",
    "    return df, target_col\n",
    "\n",
    "def load_and_process_data(config: Dict[str, Any]) -> DataInfo:\n",
    "    \"\"\"Load and preprocess the data.\"\"\"\n",
    "    print(\"\\nLoading and processing data...\")\n",
    "    \n",
    "    # Validate and get data with correct target column\n",
    "    df, original_target_col = validate_and_process_data(config['data_path'])\n",
    "    \n",
    "    # Process data\n",
    "    dataset, tensor_data, scaled_data, scaler, n_features = process(config['data_path'])\n",
    "    \n",
    "    # Validate processed data\n",
    "    validate_data(scaled_data)\n",
    "    \n",
    "    # Print information\n",
    "    print(\"\\nDataset Information:\")\n",
    "    print(f\"Original shape: {scaled_data.shape}\")\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(f\"Original target column: '{original_target_col}' (renamed to 'target')\")\n",
    "    print(f\"Control samples: {(scaled_data['target'] == 0).sum()}\")\n",
    "    print(f\"COVID-19 cases: {(scaled_data['target'] == 1).sum()}\")\n",
    "    \n",
    "    return DataInfo(dataset, tensor_data, scaled_data, scaler, n_features)\n",
    "\n",
    "\n",
    "\n",
    "def validate_synthetic_data(synthetic_data: pd.DataFrame, original_data: pd.DataFrame) -> None:\n",
    "    \"\"\"Validate synthetic data requirements.\"\"\"\n",
    "    assert len(synthetic_data) > 0, \"No synthetic samples generated\"\n",
    "    assert synthetic_data.shape[1] == original_data.shape[1], \"Feature mismatch\"\n",
    "    assert not synthetic_data.isnull().any().any(), \"Synthetic data contains NaN values\"\n",
    "    \n",
    "    # Check value ranges\n",
    "    for col in synthetic_data.columns:\n",
    "        if col not in ['data_type', 'fold', 'sample_id', 'target', 'split']:\n",
    "            orig_range = original_data[col].max() - original_data[col].min()\n",
    "            synth_range = synthetic_data[col].max() - synthetic_data[col].min()\n",
    "            assert abs(orig_range - synth_range) / orig_range < 0.5, f\"Large range discrepancy in {col}\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def train_and_evaluate_model(config: Dict[str, Any]) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Train WGAN-GP and evaluate synthetic data, with real-time loss visualization.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary containing model parameters and paths\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - synthetic_data: Generated synthetic samples\n",
    "        - original_data: Original training data\n",
    "        - results: Evaluation metrics and results\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining WGAN-GP and generating synthetic samples...\")\n",
    "    \n",
    "    # Set up the plotting environment\n",
    "    plt.ion()  # Turn on interactive mode for live updates\n",
    "    fig, ax = plt.subplots()\n",
    "    g_loss_plt, = ax.plot([], [], label='Generator Loss')\n",
    "    d_loss_plt, = ax.plot([], [], label='Discriminator Loss')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Initialize lists to store loss values\n",
    "    g_losses, d_losses = [], []\n",
    "\n",
    "    # Assume the number of epochs is part of the configuration\n",
    "    num_epochs = config['model_params']['epochs']\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train model and generate data for one epoch\n",
    "        g_loss, d_loss = train_and_generate(\n",
    "            filepath=config['data_path'],\n",
    "            save_info=True,\n",
    "            **config['model_params']\n",
    "        )\n",
    "\n",
    "        # Append the losses for plotting\n",
    "        g_losses.append(g_loss)\n",
    "        d_losses.append(d_loss)\n",
    "\n",
    "        # Update the plot data\n",
    "        g_loss_plt.set_data(range(len(g_losses)), g_losses)\n",
    "        d_loss_plt.set_data(range(len(d_losses)), d_losses)\n",
    "        \n",
    "        # Adjust plot limits\n",
    "        ax.relim()  # Recalculate limits\n",
    "        ax.autoscale_view(True, True, True)  # Rescale the view based on the limits\n",
    "        \n",
    "        # Draw and pause to update the plot\n",
    "        plt.draw()\n",
    "        plt.pause(0.1)  # Short pause to allow plot updates\n",
    "\n",
    "    # Disable interactive mode once training is complete to finalize the plot\n",
    "    plt.ioff()\n",
    "\n",
    "    # Validate synthetic data\n",
    "    validate_synthetic_data(g_losses, d_losses)\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = SyntheticDataEvaluator(output_dir=config['results_dir'])\n",
    "    \n",
    "    # Perform evaluation\n",
    "    results = evaluator.evaluate_synthetic_data(\n",
    "        original_data=g_losses,  # Example usage; adjust as needed\n",
    "        synthetic_data=d_losses,  # Example usage; adjust as needed\n",
    "        recenter=True\n",
    "    )\n",
    "    \n",
    "    # Add original and synthetic data to results\n",
    "    results['original_data'] = g_losses  # Example usage; adjust as needed\n",
    "    results['synthetic_data'] = d_losses  # Example usage; adjust as needed\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_evaluation_metrics(config: Dict[str, Any], results: Dict[str, Any]) -> None:\n",
    "    \"\"\"Generate and save evaluation plots.\"\"\"\n",
    "    print(\"\\nGenerating evaluation visualizations...\")\n",
    "    \n",
    "    # Ensure results contain required keys\n",
    "    required_keys = ['distribution_comparison', 'synthetic_data', 'original_data']\n",
    "    assert all(key in results for key in required_keys), \"Missing required results\"\n",
    "    \n",
    "    # KS Statistics Distribution\n",
    "    plt.figure(figsize=config['visualization_params']['figsize'])\n",
    "    sns.histplot(\n",
    "        data=results['distribution_comparison'],\n",
    "        x='KS_Statistic',\n",
    "        kde=True,\n",
    "        bins=20\n",
    "    )\n",
    "    plt.title('Distribution of KS Statistics')\n",
    "    plt.xlabel('KS Statistic')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(config['results_dir'] / \"ks_statistics_distribution.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature-wise Comparisons\n",
    "    recentered_synthetic = results['synthetic_data']\n",
    "    original_data = results['original_data']\n",
    "    \n",
    "    feature_cols = [col for col in original_data.columns \n",
    "                   if col not in ['data_type', 'fold', 'sample_id', 'target', 'split']]\n",
    "    \n",
    "    for column in feature_cols:\n",
    "        plt.figure(figsize=config['visualization_params']['figsize'])\n",
    "        \n",
    "        # Plot distributions\n",
    "        sns.kdeplot(\n",
    "            data=original_data[column],\n",
    "            label='Original',\n",
    "            alpha=0.7\n",
    "        )\n",
    "        sns.kdeplot(\n",
    "            data=recentered_synthetic[column],\n",
    "            label='Synthetic (Recentered)',\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "        # Add statistics\n",
    "        orig_mean = original_data[column].mean()\n",
    "        synth_mean = recentered_synthetic[column].mean()\n",
    "        \n",
    "        plt.axvline(orig_mean, color='blue', linestyle='--', alpha=0.5,\n",
    "                   label=f'Original Mean: {orig_mean:.2f}')\n",
    "        plt.axvline(synth_mean, color='orange', linestyle='--', alpha=0.5,\n",
    "                   label=f'Synthetic Mean: {synth_mean:.2f}')\n",
    "        \n",
    "        plt.title(f'Distribution Comparison: {column}')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig(\n",
    "            config['results_dir'] / f\"feature_distribution_{column}.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "def generate_summary_report(config: Dict[str, Any], results: Dict[str, Any]) -> None:\n",
    "    \"\"\"Generate and save comprehensive evaluation report.\"\"\"\n",
    "    # Validate results\n",
    "    assert 'distribution_comparison' in results, \"Missing distribution comparison results\"\n",
    "    \n",
    "    report = {\n",
    "        \"Evaluation Summary\": {\n",
    "            \"Original Samples\": len(results['original_data']),\n",
    "            \"Synthetic Samples\": len(results['synthetic_data']),\n",
    "            \"Features\": len(results['original_data'].columns),\n",
    "            \"Recentering Applied\": results['recentering_applied']\n",
    "        },\n",
    "        \"Quality Metrics\": {\n",
    "            \"Mean KS Statistic\": results['distribution_comparison']['KS_Statistic'].mean(),\n",
    "            \"Median KS Statistic\": results['distribution_comparison']['KS_Statistic'].median(),\n",
    "            \"Features with Good Match (KS < 0.1)\": (results['distribution_comparison']['KS_Statistic'] < 0.1).sum(),\n",
    "            \"Percentage of Well-matched Features\": f\"{(results['distribution_comparison']['KS_Statistic'] < 0.1).mean()*100:.2f}%\"\n",
    "        },\n",
    "        \"Training Parameters\": config['model_params']\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    report_path = config['results_dir'] / \"evaluation_report.txt\"\n",
    "    with open(report_path, \"w\", encoding='utf-8') as f:\n",
    "        for section, content in report.items():\n",
    "            f.write(f\"\\n{section}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            for key, value in content.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "def main() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main execution function for WGAN-GP training and evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing results and evaluation metrics\n",
    "    \"\"\"\n",
    "    # 1. Configuration Setup\n",
    "    exploration_config = setup_config()\n",
    "    print_config(exploration_config)\n",
    "    \n",
    "    # 2. Data Loading and Preprocessing\n",
    "    print(\"\\nStep 1: Data Loading and Preprocessing\")\n",
    "    print(\"-\" * 50)\n",
    "    data_info = load_and_process_data(exploration_config)\n",
    "    \n",
    "    # 3. Model Training and Evaluation\n",
    "    print(\"\\nStep 2: Model Training and Evaluation\")\n",
    "    print(\"-\" * 50)\n",
    "    results = train_and_evaluate_model(exploration_config)\n",
    "    \n",
    "    # 4. Store Dataset Information\n",
    "    results['data_info'] = {\n",
    "        'n_features': data_info.n_features,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'original_samples': len(data_info.scaled_data),\n",
    "        'control_samples': (data_info.scaled_data['target'] == 0).sum(),\n",
    "        'covid_samples': (data_info.scaled_data['target'] == 1).sum()\n",
    "    }\n",
    "    \n",
    "    # 5. Generate Visualizations\n",
    "    print(\"\\nStep 3: Generating Visualizations\")\n",
    "    print(\"-\" * 50)\n",
    "    plot_evaluation_metrics(exploration_config, results)\n",
    "    \n",
    "    # 6. Generate Report\n",
    "    print(\"\\nStep 4: Generating Summary Report\")\n",
    "    print(\"-\" * 50)\n",
    "    generate_summary_report(exploration_config, results)\n",
    "    \n",
    "    # 7. Final Summary\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Results saved in: {exploration_config['results_dir']}\")\n",
    "    print(f\"Total samples processed: {len(data_info.scaled_data)}\")\n",
    "    print(f\"Features analyzed: {data_info.n_features}\")\n",
    "    print(f\"Synthetic samples generated: {len(results['synthetic_data'])}\")\n",
    "    \n",
    "    # Return results for interactive analysis\n",
    "    return {\n",
    "        'config': exploration_config,\n",
    "        'data_info': data_info,\n",
    "        'results': results,\n",
    "        'evaluation_metrics': {\n",
    "            'ks_stats': results['distribution_comparison'],\n",
    "            'feature_matches': (results['distribution_comparison']['KS_Statistic'] < 0.1).sum(),\n",
    "            'quality_score': (results['distribution_comparison']['KS_Statistic'] < 0.1).mean() * 100\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Execute if running as script\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(config.RANDOM_STATE)\n",
    "    np.random.seed(config.RANDOM_STATE)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Run main function\n",
    "    analysis_results = main()\n",
    "    \n",
    "    # Print final quality score\n",
    "    quality_score = analysis_results['evaluation_metrics']['quality_score']\n",
    "    print(f\"\\nFinal Quality Score: {quality_score:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
