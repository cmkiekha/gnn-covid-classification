import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from scipy.stats import ks_2samp
from sklearn.manifold import TSNE
from typing import List, Optional
from sklearn.preprocessing import RobustScaler


def recenter_data(
    generated_samples: np.ndarray,
    original_data: np.ndarray,
    scaler: Optional[object] = None,
    epsilon: float = 1e-10,
) -> np.ndarray:
    """
    Adjusts the generated samples to match the statistical properties (mean and standard deviation)
    of the original data, ensuring that the synthetic data mimics the real data's distribution more closely.

    Parameters:
    - generated_samples: np.ndarray, synthetic samples generated by the model.
    - original_data: np.ndarray, real data samples to match.
    - scaler: Optional[object], a scaler (e.g., StandardScaler, RobustScaler) used for scaling.
    - epsilon: float, small value to prevent division by zero.

    Returns:
    - np.ndarray: Re-centered synthetic samples.
    """
    # Apply scaling if a scaler is provided
    if scaler is not None and hasattr(scaler, "transform"):
        generated_samples = scaler.transform(generated_samples)
        original_data = scaler.transform(original_data)

    # Calculate the mean and standard deviation for both original and generated data
    gen_mean = generated_samples.mean(axis=0)
    gen_std = generated_samples.std(axis=0) + epsilon
    orig_mean = original_data.mean(axis=0)
    orig_std = original_data.std(axis=0) + epsilon

    # Re-center the generated samples to match the original data's distribution
    centered_data = (generated_samples - gen_mean) / gen_std * orig_std + orig_mean

    # If a scaler is provided, reverse the scaling to get the final output
    if scaler is not None and hasattr(scaler, "inverse_transform"):
        centered_data = scaler.inverse_transform(centered_data)

    return centered_data


def compare_statistics(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
    """
    Compares the statistical properties between original and synthetic datasets,
    calculating differences in means and variances for each feature.
    """
    stats = {
        "Feature": [],
        "Mean Difference": [],
        "Variance Difference": [],
        "Relative Mean Diff %": [],
        "Relative Variance Diff %": [],
    }

    # Make sure these columns are NOT being scaled
    cols = [
        c
        for c in df1.select_dtypes(include=[np.number]).columns
        if c not in ("fold", "data_type")
    ]

    for col in cols:
        mean_diff = df1[col].mean() - df2[col].mean()
        variance_diff = df1[col].var() - df2[col].var()
        stats["Feature"].append(col)
        stats["Mean Difference"].append(mean_diff)
        stats["Variance Difference"].append(variance_diff)
        stats["Relative Mean Diff %"].append(
            mean_diff / df1[col].mean() * 100 if df1[col].mean() else 0
        )
        stats["Relative Variance Diff %"].append(
            variance_diff / df1[col].var() * 100 if df1[col].var() else 0
        )
    return pd.DataFrame(stats)


def compare_distributions(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
    """
    Performs the Kolmogorov-Smirnov test for each feature to compare distributions
    between original and synthetic data.
    """
    results = {
        "Feature": [],
        "KS Statistic": [],
        "P-Value": [],
        "Distribution Match": [],
    }

    cols = [
        c
        for c in df1.select_dtypes(include=[np.number]).columns
        if c not in ("fold", "data_type")
    ]

    for col in cols:
        ks_stat, p_value = ks_2samp(df1[col], df2[col])
        results["Feature"].append(col)
        results["KS Statistic"].append(ks_stat)
        results["P-Value"].append(p_value)
        results["Distribution Match"].append(
            "Similar" if p_value > 0.05 else "Different"
        )
    return pd.DataFrame(results)


def generate_tsne(
    df1: pd.DataFrame, df2: pd.DataFrame, perplexity: int = 30, n_iter: int = 1000
):
    """
    Generates a t-SNE visualization to compare the feature spaces of original and synthetic data.
    """
    combined_df = pd.concat([df1.assign(Type="Original"), df2.assign(Type="Synthetic")])

    cols = [
        c
        for c in combined_df.select_dtypes(include=[np.number]).columns
        if c not in ("fold", "data_type")
    ]

    combined_df = combined_df[cols]

    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter, random_state=42)
    tsne_results = tsne.fit_transform(combined_df)
    tsne_df = pd.DataFrame(tsne_results, columns=["TSNE-1", "TSNE-2"])
    # tsne_df["Type"] = combined_df["Type"]

    plt.figure(figsize=(10, 6))
    # figure out how to add the "class/type"
    sns.scatterplot(x="TSNE-1", y="TSNE-2", data=tsne_df, palette=["blue", "red"])
    plt.title("t-SNE Visualization of Original vs Synthetic Data")
    plt.show()


def plot_feature_distributions(
    original_df: pd.DataFrame,
    synthetic_df: pd.DataFrame,
    features: List[str] = None,
    max_features: int = 10,
):
    """
    Creates side-by-side distribution plots for selected features from original and synthetic datasets.
    """
    if features is None:
        features = original_df.columns[:max_features]
    n_cols = 2
    n_rows = (len(features) + 1) // 2
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 5 * n_rows))
    axes = axes.ravel()

    for i, feature in enumerate(features):
        sns.kdeplot(original_df[feature], ax=axes[i], label="Original", color="blue")
        sns.kdeplot(synthetic_df[feature], ax=axes[i], label="Synthetic", color="red")
        axes[i].set_title(f"Distribution of {feature}")
        axes[i].legend()

    plt.tight_layout()
    plt.show()
