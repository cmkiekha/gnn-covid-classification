# evaluation_v2.py
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from pathlib import Path
from typing import List, Dict, Any
from scipy.stats import ks_2samp
from sklearn.manifold import TSNE

class SyntheticDataEvaluator:
    """
    Comprehensive evaluation suite for synthetic data quality assessment.
    Handles statistical comparisons, visualization, and result saving.
    """
    
    def __init__(self, output_dir: str):
        """Initialize evaluator with output directory."""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    @staticmethod
    def recenter_data(
        generated_samples: np.ndarray, 
        original_data: np.ndarray, 
        epsilon: float = 1e-10
    ) -> np.ndarray:
        """
        Adjusts the generated samples to match the statistical properties 
        (mean and standard deviation) of the original data.

        This recentering ensures that the synthetic data mimics the real data's
        distribution more closely while maintaining the underlying patterns.

        Args:
            generated_samples (np.ndarray): The synthetic data generated by the model.
            original_data (np.ndarray): The real data used as the reference.
            epsilon (float): Small constant to prevent division by zero.

        Returns:
            np.ndarray: Recentered synthetic data.

        Raises:
            ValueError: If input dimensions don't match or contain invalid values.
        """
        # Validate inputs
        if generated_samples.shape[1] != original_data.shape[1]:
            raise ValueError(
                f"Feature dimension mismatch: {generated_samples.shape[1]} vs {original_data.shape[1]}"
            )

        if not np.isfinite(generated_samples).all() or not np.isfinite(original_data).all():
            raise ValueError("Input arrays contain inf or nan values")

        # Calculate statistics
        gen_mean = np.mean(generated_samples, axis=0)
        gen_std = np.std(generated_samples, axis=0) + epsilon
        orig_mean = np.mean(original_data, axis=0)
        orig_std = np.std(original_data, axis=0) + epsilon

        # Standardize and rescale
        standardized = (generated_samples - gen_mean) / gen_std
        recentered = (standardized * orig_std) + orig_mean

        # Verify recentering
        if not np.isfinite(recentered).all():
            raise ValueError("Recentering produced invalid values")

        return recentered

    def evaluate_synthetic_data(self, 
                              original_data: pd.DataFrame, 
                              synthetic_data: pd.DataFrame,
                              recenter: bool = True) -> Dict[str, Any]:
        """
        Comprehensive evaluation of synthetic data quality.
        
        Args:
            original_data: Original dataset
            synthetic_data: Generated synthetic dataset
            recenter: Whether to recenter the synthetic data before evaluation
        
        Returns:
            Dict containing evaluation results
        """
        # Store original data
        self.original_data = original_data.copy()
        
        # Recenter synthetic data if requested
        if recenter:
            synthetic_features = [col for col in synthetic_data.columns 
                                if col not in ['data_type', 'fold', 'sample_id', 'type']]
            
            recentered_data = self.recenter_data(
                synthetic_data[synthetic_features].values,
                original_data[synthetic_features].values
            )
            
            # Create recentered DataFrame
            self.synthetic_data = pd.DataFrame(
                recentered_data,
                columns=synthetic_features,
                index=synthetic_data.index
            )
            
            # Add back metadata columns
            for col in synthetic_data.columns:
                if col not in synthetic_features:
                    self.synthetic_data[col] = synthetic_data[col]
        else:
            self.synthetic_data = synthetic_data.copy()
        
        # Perform evaluations
        results = {
            'statistical_comparison': self.compare_statistics(
                self.original_data, 
                self.synthetic_data
            ),
            'distribution_comparison': self.compare_distributions(
                self.original_data, 
                self.synthetic_data
            ),
            'original_data': self.original_data,
            'synthetic_data': self.synthetic_data,
            'recentering_applied': recenter
        }
        
        # Generate and save plots
        self.plot_evaluation_results(results)
        
        # Save numerical results
        self.save_results(results)
        
        return results

    def compare_statistics(self, df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
        """
        Compare statistical properties between original and synthetic data.
        
        Args:
            df1: Original dataframe
            df2: Synthetic dataframe
        
        Returns:
            DataFrame with statistical comparisons
        """
        comparison_dict = {
            "Feature": [],
            "Mean_Difference": [],
            "Variance_Difference": [],
            "Relative_Mean_Diff_%": [],
            "Relative_Var_Diff_%": [],
            "Original_Mean": [],
            "Synthetic_Mean": [],
            "Original_Std": [],
            "Synthetic_Std": []
        }

        for column in df1.columns:
            if column not in ['data_type', 'fold', 'sample_id', 'type']:
                original_mean = df1[column].mean()
                synthetic_mean = df2[column].mean()
                original_std = df1[column].std()
                synthetic_std = df2[column].std()
                original_var = df1[column].var()
                synthetic_var = df2[column].var()

                mean_diff = original_mean - synthetic_mean
                var_diff = original_var - synthetic_var
                rel_mean_diff = (mean_diff / original_mean * 100) if original_mean != 0 else np.inf
                rel_var_diff = (var_diff / original_var * 100) if original_var != 0 else np.inf

                comparison_dict["Feature"].append(column)
                comparison_dict["Mean_Difference"].append(mean_diff)
                comparison_dict["Variance_Difference"].append(var_diff)
                comparison_dict["Relative_Mean_Diff_%"].append(rel_mean_diff)
                comparison_dict["Relative_Var_Diff_%"].append(rel_var_diff)
                comparison_dict["Original_Mean"].append(original_mean)
                comparison_dict["Synthetic_Mean"].append(synthetic_mean)
                comparison_dict["Original_Std"].append(original_std)
                comparison_dict["Synthetic_Std"].append(synthetic_std)

        return pd.DataFrame(comparison_dict)

    def compare_distributions(self, df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
        """
        Perform KS tests between original and synthetic distributions.
        
        Args:
            df1: Original dataframe
            df2: Synthetic dataframe
        
        Returns:
            DataFrame with KS test results
        """
        ks_results = {
            "Feature": [],
            "KS_Statistic": [],
            "P_Value": [],
            "Distribution_Match": []
        }

        for column in df1.columns:
            if column not in ['data_type', 'fold', 'sample_id', 'type']:
                statistic, pvalue = ks_2samp(df1[column], df2[column])
                ks_results["Feature"].append(column)
                ks_results["KS_Statistic"].append(statistic)
                ks_results["P_Value"].append(pvalue)
                ks_results["Distribution_Match"].append("Similar" if pvalue > 0.05 else "Different")

        return pd.DataFrame(ks_results).sort_values("KS_Statistic", ascending=False)

    def plot_evaluation_results(self, results: Dict[str, Any]) -> None:
        """Generate and save all evaluation plots."""
        self._plot_ks_statistics(results['distribution_comparison'])
        self._plot_feature_distributions()
        self._plot_tsne_visualization()
        self._plot_statistical_comparisons(results['statistical_comparison'])
    
    def _plot_ks_statistics(self, ks_results: pd.DataFrame) -> None:
        """Plot KS statistics distribution."""
        plt.figure(figsize=(10, 6))
        sns.histplot(data=ks_results, x='KS_Statistic', kde=True)
        plt.title('Distribution of KS Statistics: Original vs Synthetic Data')
        plt.xlabel('KS Statistic')
        plt.ylabel('Frequency')
        plt.grid(True)
        plt.savefig(self.output_dir / 'ks_statistics.png')
        plt.close()

    def _plot_feature_distributions(self) -> None:
        """Plot feature-wise distributions."""
        for column in self.original_data.columns:
            if column not in ['data_type', 'fold', 'sample_id', 'type']:
                plt.figure(figsize=(10, 6))
                sns.kdeplot(data=self.original_data[column], label='Original', alpha=0.6)
                sns.kdeplot(data=self.synthetic_data[column], label='Synthetic', alpha=0.6)
                plt.title(f'Distribution Comparison: {column}')
                plt.xlabel('Value')
                plt.ylabel('Density')
                plt.legend()
                plt.grid(True)
                plt.savefig(self.output_dir / f'feature_distribution_{column}.png')
                plt.close()

    def _plot_tsne_visualization(self) -> None:
        """Generate and save t-SNE visualization."""
        # Prepare data
        df1_copy = self.original_data.copy()
        df2_copy = self.synthetic_data.copy()
        df1_copy['type'] = 'Original'
        df2_copy['type'] = 'Synthetic'
        
        combined_df = pd.concat([df1_copy, df2_copy])
        features = [col for col in combined_df.columns 
                   if col not in ['data_type', 'fold', 'sample_id', 'type']]
        
        # Compute t-SNE
        tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)
        tsne_results = tsne.fit_transform(combined_df[features])
        
        # Plot
        plt.figure(figsize=(10, 10))
        sns.scatterplot(
            x=tsne_results[:, 0],
            y=tsne_results[:, 1],
            hue=combined_df['type'],
            palette=sns.color_palette("hsv", 2),
            alpha=0.7
        )
        plt.title('t-SNE Visualization: Original vs Synthetic Data')
        plt.savefig(self.output_dir / 'tsne_visualization.png')
        plt.close()

    def save_results(self, results: Dict[str, Any]) -> None:
        """Save numerical results to files."""
        # Save statistical comparisons
        results['statistical_comparison'].to_csv(
            self.output_dir / 'statistical_comparison.csv', index=False)
        
        # Save KS test results
        results['distribution_comparison'].to_csv(
            self.output_dir / 'distribution_comparison.csv', index=False)
        
        # Save summary metrics
        summary = {
            'n_original_samples': len(self.original_data),
            'n_synthetic_samples': len(self.synthetic_data),
            'n_features': len(self.original_data.columns),
            'mean_ks_statistic': results['distribution_comparison']['KS_Statistic'].mean(),
            'percent_similar_distributions': 
                (results['distribution_comparison']['Distribution_Match'] == 'Similar').mean() * 100
        }
        
        with open(self.output_dir / 'evaluation_summary.json', 'w') as f:
            json.dump(summary, f, indent=4)

def evaluate_synthetic_data(original_data: pd.DataFrame, 
                          synthetic_data: pd.DataFrame,
                          output_dir: str) -> Dict[str, Any]:
    """
    Wrapper function for synthetic data evaluation.
    
    Args:
        original_data: Original dataset
        synthetic_data: Generated synthetic dataset
        output_dir: Directory for saving results
    
    Returns:
        Dict containing evaluation results
    """
    evaluator = SyntheticDataEvaluator(output_dir)
    return evaluator.evaluate_synthetic_data(original_data, synthetic_data)
```
